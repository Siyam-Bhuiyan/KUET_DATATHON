{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install Optuna\n",
    "!pip install optuna\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import StackingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "# Load your data\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# Preprocessing (drop columns and separate target)\n",
    "drop_columns = ['address', 'educational_institution_name', 'degree_names', 'passing_years', 'educational_results', 'result_types', 'major_field_of_studies']\n",
    "X = train.drop(columns=drop_columns + ['matched_score'])\n",
    "y = train['matched_score']\n",
    "X_test = test.drop(columns=drop_columns)\n",
    "\n",
    "# Identify text columns\n",
    "text_columns = [col for col in X.columns if X[col].dtype == 'object']\n",
    "num_columns = X.select_dtypes(exclude=['object']).columns.tolist()\n",
    "\n",
    "# Function to concatenate text data for Tfidf processing\n",
    "def combine_text_columns(data_frame, to_combine):\n",
    "    \"\"\" Combines all text columns into a single column \"\"\"\n",
    "    text_data = data_frame[to_combine].astype(str)\n",
    "    text_data = text_data.apply(lambda x: ' '.join(x), axis=1)\n",
    "    return text_data\n",
    "\n",
    "# Preprocessing pipelines for numerical and text data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())]), num_columns),\n",
    "        ('txt', Pipeline([('combine', FunctionTransformer(combine_text_columns, kw_args={'to_combine': text_columns})), ('vectorizer', TfidfVectorizer())]), text_columns)\n",
    "    ],\n",
    "    remainder='passthrough'  # Ensure no column is dropped unintentionally\n",
    ")\n",
    "\n",
    "# Apply preprocessing\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(\"Processed feature shape:\", X_processed.shape)\n",
    "print(\"Target shape:\", y.shape)\n",
    "\n",
    "# Ensure data consistency\n",
    "assert X_processed.shape[0] == y.shape[0], \"Mismatch in the number of samples between features and target\"\n",
    "\n",
    "\n",
    "\n",
    "# Variable to track the lowest MSE\n",
    "lowest_mse = np.inf\n",
    "\n",
    "# Optuna optimization for Gradient Boosting\n",
    "def objective(trial):\n",
    "    global lowest_mse\n",
    "    gbm_params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "    }\n",
    "    gbm = GradientBoostingRegressor(**gbm_params)\n",
    "    score = cross_val_score(gbm, X_processed, y, n_jobs=-1, cv=5, scoring='neg_mean_squared_error').mean()\n",
    "    mse = -score  # Negate to get positive MSE\n",
    "    lowest_mse = min(lowest_mse, mse)\n",
    "    print(f'Trial {trial.number}: MSE = {mse}, Lowest MSE so far = {lowest_mse}')\n",
    "    return mse\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Best hyperparameters and lowest MSE ever faced\n",
    "best_gbm_params = study.best_params\n",
    "print(\"Best hyperparameters for Gradient Boosting:\", best_gbm_params)\n",
    "print(\"Lowest MSE encountered during the study:\", lowest_mse)\n",
    "\n",
    "# Set up the optimized GBM in the stacking ensemble\n",
    "estimators = [\n",
    "    ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),\n",
    "    ('gbm', GradientBoostingRegressor(**best_gbm_params)),\n",
    "    ('svr', make_pipeline(StandardScaler(with_mean=False), SVR(C=1, epsilon=0.1)))  # Fix with_mean=False\n",
    "]\n",
    "stack_reg = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
    ")\n",
    "\n",
    "# Train the stacked model\n",
    "stack_reg.fit(X_processed, y)\n",
    "\n",
    "# Prepare final predictions for submission\n",
    "final_predictions = stack_reg.predict(X_test_processed)\n",
    "\n",
    "# Save submission\n",
    "sample_submission['matched_score'] = final_predictions\n",
    "sample_submission.to_csv('finsubmission.csv', index=False)\n",
    "print(\"Final submission saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
